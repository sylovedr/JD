2023-04-19 16:18:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: JD)
2023-04-19 16:18:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 22:45:29) [MSC v.1916 32 bit (Intel)], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Windows-10-10.0.19041-SP0
2023-04-19 16:18:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'JD',
 'DOWNLOAD_DELAY': 1,
 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'D:\\liuwei\\study\\scrapy\\JD\\JD\\crawler.log',
 'NEWSPIDER_MODULE': 'JD.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler',
 'SPIDER_MODULES': ['JD.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-04-19 16:18:30 [asyncio] DEBUG: Using selector: SelectSelector
2023-04-19 16:18:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-04-19 16:18:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2023-04-19 16:18:30 [scrapy.extensions.telnet] INFO: Telnet Password: 118d407a2f3c1ece
2023-04-19 16:18:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-04-19 16:18:30 [book] INFO: Reading start URLs from redis key 'py21' (batch size: 16, encoding: utf-8)
2023-04-19 16:18:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-04-19 16:18:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-04-19 16:18:31 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline']
2023-04-19 16:18:31 [scrapy.core.engine] INFO: Spider opened
2023-04-19 16:18:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-04-19 16:18:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-04-19 16:18:36 [book] WARNING: [93mWARNING: String request is deprecated, please use JSON data format.                 Detail information, please check https://github.com/rmax/scrapy-redis#features[0m
2023-04-19 16:18:36 [py.warnings] WARNING: D:\liuwei\study\scrapy\my_scrapy\enve\lib\site-packages\scrapy_redis\spiders.py:197: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.crawl is deprecated
  self.crawler.engine.crawl(req, spider=self)

2023-04-19 16:18:36 [book] DEBUG: Read 1 requests from 'py21'
2023-04-19 16:18:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pjapi.jd.com/book/sort?source=bookSort> (referer: https://book.jd.com/)
2023-04-19 16:18:36 [py.warnings] WARNING: D:\liuwei\study\scrapy\my_scrapy\enve\lib\site-packages\scrapy_redis\dupefilter.py:115: ScrapyDeprecationWarning: Call to deprecated function scrapy.utils.request.request_fingerprint().

If you are using this function in a Scrapy component, and you are OK with users of your component changing the fingerprinting algorithm through settings, use crawler.request_fingerprinter.fingerprint() instead in your Scrapy component (you can get the crawler object from the 'from_crawler' class method).

Otherwise, consider using the scrapy.utils.request.fingerprint() function instead.

Either way, the resulting fingerprints will be returned as bytes, not as a string, and they will also be different from those generated by 'request_fingerprint()'. Before you switch, make sure that you understand the consequences of this (e.g. cache invalidation) and are OK with them; otherwise, consider implementing your own function which returns the same fingerprints as the deprecated 'request_fingerprint()' function.
  return request_fingerprint(request)

2023-04-19 16:18:36 [scrapy_redis.dupefilter] DEBUG: Filtered duplicate request <GET https://list.jd.com/list.html?cat=1713,3297> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-04-19 16:19:31 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-04-19 16:20:31 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-04-19 16:20:40 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: JD)
2023-04-19 16:20:40 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 22:45:29) [MSC v.1916 32 bit (Intel)], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Windows-10-10.0.19041-SP0
2023-04-19 16:20:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'JD',
 'DOWNLOAD_DELAY': 1,
 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'D:\\liuwei\\study\\scrapy\\JD\\JD\\crawler.log',
 'NEWSPIDER_MODULE': 'JD.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler',
 'SPIDER_MODULES': ['JD.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2023-04-19 16:20:40 [asyncio] DEBUG: Using selector: SelectSelector
2023-04-19 16:20:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2023-04-19 16:20:40 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2023-04-19 16:20:40 [scrapy.extensions.telnet] INFO: Telnet Password: 48f341e92cea4792
2023-04-19 16:20:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2023-04-19 16:20:40 [book] INFO: Reading start URLs from redis key 'py21' (batch size: 16, encoding: utf-8)
2023-04-19 16:20:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-04-19 16:20:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-04-19 16:20:40 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline']
2023-04-19 16:20:40 [scrapy.core.engine] INFO: Spider opened
2023-04-19 16:20:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-04-19 16:20:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-04-19 16:21:10 [book] WARNING: [93mWARNING: String request is deprecated, please use JSON data format.                 Detail information, please check https://github.com/rmax/scrapy-redis#features[0m
2023-04-19 16:21:10 [py.warnings] WARNING: D:\liuwei\study\scrapy\my_scrapy\enve\lib\site-packages\scrapy_redis\spiders.py:197: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.crawl is deprecated
  self.crawler.engine.crawl(req, spider=self)

2023-04-19 16:21:10 [book] DEBUG: Read 1 requests from 'py21'
2023-04-19 16:21:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pjapi.jd.com/book/sort?source=bookSort> (referer: https://book.jd.com/)
2023-04-19 16:21:10 [py.warnings] WARNING: D:\liuwei\study\scrapy\my_scrapy\enve\lib\site-packages\scrapy_redis\dupefilter.py:115: ScrapyDeprecationWarning: Call to deprecated function scrapy.utils.request.request_fingerprint().

If you are using this function in a Scrapy component, and you are OK with users of your component changing the fingerprinting algorithm through settings, use crawler.request_fingerprinter.fingerprint() instead in your Scrapy component (you can get the crawler object from the 'from_crawler' class method).

Otherwise, consider using the scrapy.utils.request.fingerprint() function instead.

Either way, the resulting fingerprints will be returned as bytes, not as a string, and they will also be different from those generated by 'request_fingerprint()'. Before you switch, make sure that you understand the consequences of this (e.g. cache invalidation) and are OK with them; otherwise, consider implementing your own function which returns the same fingerprints as the deprecated 'request_fingerprint()' function.
  return request_fingerprint(request)

2023-04-19 16:21:10 [scrapy_redis.dupefilter] DEBUG: Filtered duplicate request <GET https://list.jd.com/list.html?cat=1713,3297> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2023-04-19 16:21:40 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2023-04-19 16:22:40 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-04-19 16:23:40 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
